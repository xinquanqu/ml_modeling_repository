from typing import List


from transformers import PreTrainedTokenizer


def write_logprobs(text: str, tokenizer: PreTrainedTokenizer, logprobs: List[float], file_path: str):
    """
    Write the logprobs to file.

    Args:
        text (str): The text that was used to generate the logprobs.
        tokenizer (PreTrainedTokenizer): The tokenizer used to generate the logprobs.
        logprobs (List[float]): The logprobs generated by the model.
        file_path (str): The file path to write the logprobs to.
    """
    tokens = tokenizer.convert_ids_to_tokens(tokenizer.encode(text))

    to_write = ""
    # TODO: Remove negative size here and have them figure out the issue
    for _, (token, logprob) in enumerate(zip(tokens, logprobs)):
        to_write += f"{token} {-logprob}\n"

    with open(file_path, "w") as f:
        f.write(to_write)